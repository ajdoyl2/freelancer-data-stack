#!/bin/bash

# Freelancer Data Stack Setup Script
# This script initializes the data stack environment

set -e

echo "ğŸš€ Setting up Freelancer Data Stack..."

# Check if Docker is running
if ! docker info > /dev/null 2>&1; then
    echo "âŒ Docker is not running. Please start Docker Desktop and try again."
    exit 1
fi

echo "âœ… Docker is running"

# Create volumes directory structure
echo "ğŸ“ Creating volume directories..."
mkdir -p ~/data-stack/volumes/{postgres,airbyte/{workspace,data,local},airflow/{dags,logs,config,plugins},redis,datahub/{gms,frontend},elasticsearch,neo4j,kafka,zookeeper/{data,logs},great-expectations,evidence,metabase,duckdb,traefik}

echo "âœ… Volume directories created"

# Copy DAGs to Airflow volume
echo "ğŸ“‹ Setting up Airflow DAGs..."
cp -r dags/* ~/data-stack/volumes/airflow/dags/ 2>/dev/null || echo "No DAGs to copy yet"

# Check if .env file exists
if [ ! -f .env ]; then
    echo "âš ï¸  No .env file found. Copying from template..."
    cp .env.template .env
    echo "ğŸ“ Please edit .env file with your preferred passwords before starting the stack"
    echo "   Current file has secure defaults that you can use as-is"
fi

echo "âœ… Environment configuration ready"

# Make sure scripts are executable
chmod +x scripts/*.sh

echo "ğŸ³ Starting Docker Compose stack..."
echo "This may take a few minutes on first run as images are downloaded..."

# Start the stack
docker-compose up -d

echo ""
echo "ğŸ‰ Freelancer Data Stack is starting up!"
echo ""
echo "ğŸ“Š Access your services at:"
echo "   â€¢ Airflow:              http://localhost:8080"
echo "   â€¢ Airflow Flower:       http://localhost:5555" 
echo "   â€¢ Airbyte:              http://localhost:8001"
echo "   â€¢ DataHub:              http://localhost:9002"
echo "   â€¢ Metabase:             http://localhost:3002"
echo "   â€¢ Great Expectations:   http://localhost:8888"
echo "   â€¢ Evidence.dev:         http://localhost:3001"
echo "   â€¢ DuckDB HTTP:          http://localhost:8002"
echo "   â€¢ Traefik Dashboard:    http://localhost:8090"
echo ""
echo "ğŸ” Default credentials:"
echo "   â€¢ Airflow: admin / (check your .env file)"
echo "   â€¢ Jupyter: token from .env file or no token if empty"
echo ""
echo "â³ Services may take 2-3 minutes to fully initialize"
echo "ğŸ“‹ Run 'docker-compose logs -f [service-name]' to check individual service logs"
echo "ğŸ›‘ Run 'docker-compose down' to stop all services"
echo ""
echo "Happy data engineering! ğŸ¯"
