# DataHub ingestion configuration for Dagster assets
# This configuration maps Dagster assets to DataHub for metadata management

source:
  type: dagster
  config:
    # Dagster instance configuration
    host: ${DAGSTER_HOST:-localhost}
    port: ${DAGSTER_PORT:-3000}

    # Optional authentication
    # api_key: ${DAGSTER_API_KEY}

    # Asset filtering
    asset_name_pattern:
      allow:
        - ".*"  # Allow all assets

    # Job filtering
    job_name_pattern:
      allow:
        - "daily_data_pipeline"
        - "incremental_sync_pipeline"
        - "transformation_pipeline"
        - "data_quality_pipeline"
        - "release_triggered_pipeline"

    # Metadata extraction configuration
    extract_owners: true
    extract_descriptions: true
    extract_tags: true
    extract_usage_statistics: true

    # Lineage configuration
    extract_lineage: true
    lineage_job_name_pattern:
      allow:
        - ".*"

    # Custom asset mappings
    asset_mappings:
      # Map Airbyte sync job to external data sources
      airbyte_sync_job:
        platform: "airbyte"
        dataset_name: "external_data_sync"
        description: "Data extraction from external APIs via Airbyte"
        tags:
          - "ingestion"
          - "external"
          - "api"

      # Map dlt ingestion pipeline
      dlt_ingestion_pipeline:
        platform: "dlt"
        dataset_name: "dlt_ingested_data"
        description: "Data ingestion via dlt pipeline"
        tags:
          - "ingestion"
          - "pipeline"
          - "dlt"

      # Map dbt transformations
      dbt_run:
        platform: "dbt"
        dataset_name: "transformed_data"
        description: "Data transformations via dbt"
        tags:
          - "transformation"
          - "dbt"
          - "sql"

      # Map dbt tests
      dbt_test:
        platform: "dbt"
        dataset_name: "dbt_test_results"
        description: "Data quality tests via dbt"
        tags:
          - "testing"
          - "quality"
          - "dbt"

      # Map Great Expectations validation
      great_expectations_validation:
        platform: "great-expectations"
        dataset_name: "validation_results"
        description: "Data quality validation via Great Expectations"
        tags:
          - "validation"
          - "quality"
          - "great-expectations"

      # Map DataHub metadata ingestion
      datahub_metadata_ingestion:
        platform: "datahub"
        dataset_name: "metadata_catalog"
        description: "Metadata catalog updates"
        tags:
          - "catalog"
          - "metadata"
          - "datahub"

# DataHub sink configuration
sink:
  type: datahub-rest
  config:
    server: ${DATAHUB_GMS_URL:-http://localhost:8080}
    token: ${DATAHUB_GMS_TOKEN}

    # Extra headers if needed
    # extra_headers:
    #   X-Custom-Header: value

# Transformers for custom metadata enrichment
transformers:
  - type: simple_add_dataset_properties
    config:
      properties:
        created_by: "dagster-orchestration"
        environment: ${DAGSTER_ENV:-dev}
        data_stack: "freelancer-data-stack"

  - type: pattern_add_dataset_schema_tags
    config:
      tag_pattern: "pii"
      schema_pattern: ".*_(email|phone|ssn|credit_card).*"

  - type: pattern_add_dataset_domain
    config:
      domain_pattern:
        ".*ingestion.*": "Data Engineering"
        ".*transformation.*": "Analytics"
        ".*quality.*": "Data Quality"
        ".*catalog.*": "Data Governance"

# Reporting and monitoring
reporting:
  - type: datahub-usage-stats
  - type: file
    config:
      filename: /tmp/dagster_ingestion_report.json

# Advanced configuration
options:
  # Include or exclude deleted entities
  include_soft_deleted_entities: false

  # Batch size for ingestion
  batch_size: 1000

  # Enable/disable specific extractors
  extract_ownership: true
  extract_containers: true
  extract_lineage: true
  extract_usage_statistics: true

  # Custom metadata mappings
  custom_mapping:
    dagster_asset_groups:
      ingestion: "Data Ingestion"
      transformation: "Data Transformation"
      quality: "Data Quality"
      catalog: "Data Catalog"
